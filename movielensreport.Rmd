---
title: "Movielens Project Report"
author: "Komeil Mirzaaghapour"
date: "12/27/2020"
geometry: margin=2cm
urlcolor: black
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h',fig.align = 'center', fig.width = 5,fig.height = 2)
```

# Introduction:
Netflix uses a movie recommendation system to predict if a specific user would like a specific movie. The system uses ratings that each user has given to a set of movies to predict future ratings that the same user or similar users may give to new items. This Harvard Data Science Capstone project is motivated by the Netflix challenge in October, 2006. The company offered a million dollars reward to anyone who can improve their recommendation algorithm by 10%. Since the Netflix rating data is private, the rating data for developing better algorithm is generated by the [GroupLens research lab](https://grouplens.org/datasets/movielens/) with over 20 million ratings for over 27,000 movies by more than 138,000 users. To lessen the computational burden, this course project will be using the 10M version of the MovieLens dataset and split the data into `edx` and `validation` sets. The `edx` data set will be used to develop a machine learning algorithm to predict the movie ratings in the `validation` set which is around 10% of the 10M dataset. The goal of this course project is to minimize the root mean square error (RMSE) of the predicted ratings for the `validation` set. The general approach used in this report involves **baseline predictors with temporal effects** combined with **matrix factorization** to account for interactions between users and movies. 
```{r load_libs, echo = FALSE, message=FALSE}
library(tidyverse)
library(caret)
library(lubridate)
library(gridExtra)
library(knitr)
library(kableExtra)
```
```{r setup2, include=FALSE}
theme_update(# axis labels
             axis.title = element_text(size = 8),
             # tick labels
             axis.text = element_text(size = 6))
```
```{r create_edx_validation_set,echo=FALSE, message=FALSE}
# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
     semi_join(edx, by = "movieId") %>%
     semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```
# Data Analysis and Feature Selection: 
The `edx` and `validation` data sets are created by `the code in the Movielens course project`. The `edx` set contains all the users and movies in the `validation` set with `r n_distinct(edx$userId)` users and `r n_distinct(edx$movieId)` movies. Below is a quick overview of the edx data set:
```{r edx_summary, echo=FALSE, results='asis'}
as_tibble(edx)%>%head()%>%
  kable()%>%kable_styling(position = 'center')%>%
  column_spec(1,border_left = T)%>%column_spec(6,border_right = T)%>%
  row_spec(0,bold=T)
```
As you can see above, userId and movieId are numbers that represent specific users and movies. Timestamp is the number of seconds since January 1, 1970 (midnight UTC), not counting leap seconds (also known as Unix time). Both title and genres are of variable type character, providing details about each movie. Our label, rating is of variable type numeric with the following distribution:
```{r rating_plot,fig.width = 2.5,fig.height = 2,echo=FALSE}
mu <- mean(edx$rating)
edx%>%ggplot(aes(rating))+
  geom_histogram(col='darkblue',bins=10,fill='coral1')+
  scale_x_continuous(breaks = seq(0.5,5,0.5))+
  geom_vline(xintercept =mu,col='red',linetype='dashed')
```  
The red dashed line represents the overall rating average across all users and movies. It will be referred to as $\mu$ throughout this report. As we can see, the ratings range from 0.5 stars to 5 stars. 
We are going to take a closer look at userId, movieId, timestamp, and genres to carefully select our features for rating prediction. Titles are redundant with movieIds, so we will remove the title column to save memory.

## Movie and User Effects
First, we are going to focus on movieIds and userIds. Intuitively, we should see movie only effects and user only effects on ratings. We will call them baseline predictors. For example, if movie A is a Hollywood hit, then naturally it will perform better than average movies. Similarly, if user B is a harsh critic, then he/she will tend to rate lower than other users.
Below is rating count distribution for all movieIds and all userIds:
```{r,rate_count,echo=FALSE}
p1 <- edx%>%count(movieId)%>%
  ggplot(aes(n))+geom_histogram(col='blue4',bins = 50,fill='blue')+
  scale_x_log10()+xlab('movies(rating count log10)')
p2 <- edx%>%count(userId)%>%
  ggplot(aes(n))+geom_histogram(col='palegreen4',bins = 50,fill='palegreen')+
  scale_x_log10()+xlab('users(rating count log10)')
grid.arrange(p1,p2,ncol=2)
```  
As you can see, some movies are more popular than others and some users are more active than others.  
Next, let’s take a look at the average rating distribution for all movieIds and userIds:
```{r avg_rate_distribution,echo=FALSE}
p3 <- edx%>%group_by(movieId)%>%
  summarize(rating=mean(rating))%>%ggplot(aes(rating))+
  geom_histogram(col='blue4',bins = 50,fill='blue')+
  xlab('movie average rating')+
  geom_vline(xintercept =mu,col='red',linetype='dashed')
p4 <- edx%>%group_by(userId)%>%
  summarize(rating=mean(rating))%>%ggplot(aes(rating))+
  geom_histogram(col='palegreen4',bins = 50,fill='palegreen')+
  xlab('user average rating')+
  geom_vline(xintercept =mu,col='red',linetype='dashed')
grid.arrange(p3,p4,ncol=2)
```  
Again, the red dotted line is $\mu$. Both distributions, especially that of the users, appear to approximate a gaussian distribution. The mode for movies is very similar to $\mu$ while the mode for users is slightly higher than $\mu$. Movies distribution also has a longer tail for lower ratings.  
Finally, let’s select the top 6 most rated movies and top 6 most active users and perform quantile-quantile (Q-Q) normal plots to see if they approximate gaussian distributions:
```{r  qqplots,echo=FALSE}
top6_most_rated_movies<-edx%>%group_by(movieId)%>%
  summarize(n=n())%>%arrange(desc(n))%>%head()%>%.$movieId
top6_most_rated_users<-edx%>%group_by(userId)%>%
  summarize(n=n())%>%arrange(desc(n))%>%head()%>%.$userId
p5<-edx%>%filter(movieId%in%top6_most_rated_movies)%>%
  group_by(movieId)%>%mutate(rating_z=(rating-mean(rating))/sd(rating))%>%
  ggplot(aes(sample=rating_z,col=movieId))+geom_qq(col='blue4')+facet_wrap(~movieId,ncol=3)+xlab('movies')
p6<-edx%>%filter(userId%in%top6_most_rated_users)%>%
  group_by(userId)%>%mutate(rating_z=(rating-mean(rating))/sd(rating))%>%
  ggplot(aes(sample=rating_z,col=movieId))+geom_qq(col='palegreen4')+facet_wrap(~userId,ncol=3)+xlab('users')
grid.arrange(p5,p6,ncol=2)
```  
It appears that both user and movie groups approximate normal distribution, which implies ratings and these two features follow multivariate normal distribution. This means we can use linear regression to predict baseline predictors if we set userId and movieId as factors. To save computation time, we will perform this manually in the **method and analysis** section.

## Time and Genre Effects
### Time Effect
First, let’s look at the effect of time on ratings alone:
```{r t_rating,echo=FALSE, fig.width = 2.5,fig.height = 2,message=FALSE}
edx%>%mutate(date=as_datetime(timestamp))%>%
  mutate(date=round_date(date,unit = 'week'))%>%
  group_by(date)%>%summarize(rating=mean(rating))%>%
  ggplot(aes(date,rating))+geom_point()+geom_smooth()+
  xlab('date(weekly)')+
  geom_hline(yintercept = mu,col='red',linetype='dashed')
```  
Once again, the dotted red line is $\mu$. The timestamp is rounded into intervals of weeks and as you can see there are only minor temporal effects on movie ratings. But what about user specific and movie specific temporal effects? Some blockbuster movies may lose popularity as new Hollywood hits come out and some users may become harsher or softer critics over time. Generally speaking, movie temporal bias should change over longer time spans than user temporal bias because behavioral changes tend to be more temperamental. For this reason, we bin timestamp for movies into intervals of quarters (3 months period) and timestamp for users into intervals of one month. Let’s select top 6 movies rated over the longest time span and top 6 users with the longest rating history and look at their average rating over the course of time:
```{r t_u_i_rating, echo=FALSE, message=FALSE}
top_user_timespan<-edx%>%group_by(userId)%>%
  summarize(time_span = (max(timestamp)-min(timestamp)))%>%
  arrange(desc(time_span))%>%head()%>%.$userId
p7<- edx%>%mutate(date=round_date(as_datetime(timestamp),unit='month'))%>%
  filter(userId%in%top_user_timespan)%>%
  group_by(userId,date)%>%summarize(rating=mean(rating))%>%ggplot(aes(date,rating))+
  geom_point(col='palegreen4')+geom_smooth(col='palegreen4')+facet_wrap(~userId)+
  xlab('date(monthly_users)')+theme(axis.text.x = element_text(angle = 90,hjust=1))
top_movie_timespan <-movie_avg <- edx%>%group_by(movieId)%>%summarize(time_span=(max(timestamp)-min(timestamp)))%>%
  arrange(desc(time_span))%>%head()%>%.$movieId
p8<- edx%>%mutate(date=round_date(as_datetime(timestamp),unit='quarter'))%>%filter(movieId%in%top_movie_timespan)%>%
  group_by(movieId,date)%>%summarize(rating=mean(rating))%>%ggplot(aes(date,rating))+geom_point(col='blue')+
  geom_smooth(col='blue4')+facet_wrap(~movieId)+xlab('date(quarterly_movies)')+
  theme(axis.text.x = element_text(angle = 90,hjust=1))
grid.arrange(p7,p8,ncol=2)
```  
It appears that there are some temporal effects for both user and movie biases on ratings so our baseline predictors will include user specific and movie specific temporal effects.

### Genre Effect
Now let’s look at our last feature, genres. Genres can be an interactive component between users and movies. Some users may have a preference for documentaries and they tend to rate movies with this genre higher than others. That being said, although there are only 19 distinctive (including IMAX) genres, many movies contain more than just one genre and there are total `r n_distinct(edx$genres)` various genre combinations for the `edx` data set. This complicates the interaction between users and movies. Of course, we can separate the genre combinations into each row and look at individual user genre preference. However, one movie genre may contain action, romance, and comedy, not all 3 genres are always good representations of that movie. Separating genres into each row then calculating user preferences can generate false information. To confirm this suspicion, we compare the genre rating distribution for observations containing only one genre to that of all observations with genres separated into each row:
```{r genre_rating, echo=FALSE}
genres_separated<- edx%>%separate_rows(genres,sep='\\|')
basic_genre <- unique(genres_separated$genres)
p9<-edx%>%group_by(genres)%>%
  summarize(n=n(),avg=mean(rating),se=sd(rating)/sqrt(n()))%>%
  filter(genres%in%basic_genre)%>%filter(genres!='(no genres listed)')%>%mutate(genres=reorder(genres,avg))%>%
  ggplot(aes(x=genres,y=avg,ymin=avg-2*se,ymax=avg+2*se))+geom_point(col='turquoise')+
  geom_errorbar(col='slategrey')+ylab('rating_not_separated')+
  theme(axis.text.x = element_text(angle = 90,hjust=1))
p10<-genres_separated%>%group_by(genres)%>%
  summarize(n=n(),avg=mean(rating),se=sd(rating/sqrt(n())))%>%filter(genres!='(no genres listed)')%>%mutate(genres=reorder(genres,avg))%>%
  ggplot(aes(genres,avg,ymax=avg+2*se,ymin=avg-2*se))+geom_point(col='slateblue')+
  geom_errorbar(col='sienna1')+ylab('rating_separated')+
  theme(axis.text.x = element_text(angle = 90,hjust=1))
grid.arrange(p9,p10,ncol=2)
```
We notice some definite genre bias changes when we separate the genres columns. As there are too many complex components involved in the genres column, we decided to remove the genres column and focus on **matrix factorization** to detect any latent factors for interactions between users and movies.


# Method and Analysis:

## Model Approach:
After some in depth data analysis, we decided to split our prediction model into 2 components:

1)	**Baseline predictors**:
$$ B(u,i)= \mu+b_i+b_u+b_i(t)+b_u(t) $$
$B(u,i)$ stands for the baseline predictor for user u and movie i, $b_i$ is constant movie bias, $b_u$ is constant user bias, $b_i(t)$ is temporal movie bias (date in intervals of a quarter) and $b_u(t)$ is temporal user bias (date in intervals of one month).

2)	**Matrix Factorization for residuals**:
$$ R(u,i) = U_u \bullet V_i $$
$R(u,i)$ stands for the residual for user u and movie i, $U_u$ is the user specific factor vector, $V_i$ is the movie specific factor vector and $\bullet$ stands for vector dot product.

The **Final prediction model** is as follows:
$$Rating(u,i)=B(u,i) + R(u,i)$$
$Rating(u,i)$ stands for the rating for user u and movie i, $B(u,i)$ stands for the baseline predictor for user u and movie i and $R(u,i)$ stands for the residual for user u and movie i.

## Data Preprocessing:
As mentioned in the **Data Analysis and Feature Selection** section, we are going to remove genres and title columns and mutate timestamp column into 2 separate columns:

* $date_q$: date format with intervals of a quarter for $b_i(t)$ calculation.
* $date_m$: date format with intervals of a month for $b_u(t)$ calculation.
```{r data_preprocess,eval=FALSE,echo=FALSE}
# Use function preprocess() to create modified edx and validation set, rename as train and test
# see appendix for details
train <- preprocess(edx)
test <- preprocess(validation)
# rm edx and validation set to save space 
rm(edx, validation) 
gc()
```    
These transformations will be performed on both `edx` and `validation` sets.

## Baseline predictor fitting:
We will use regularization for baseline predictors to punish small sample sizes: $l_1$ for $b_i$ and $b_u$, $l_2$ for $b_i(t)$, and $l_3$ for $b_u(t)$. To optimize our regularization terms based upon minimum RMSE, we will use a *greedy approach* to fit $l_1$ first, then $l_2$, and finally $l_3$. Ideally, a *stochastic gradient descent* method would be more accurate, however, a *greedy approach* works well enough and saves computation time. Each fitting will use *cross validation* over 5 different sets of train(~90%) and test data (~10%) splitting using the `edx` set. The partition will be done in a way that guarantees the train set contains all users and movies in the corresponding test set.

Steps are outlined below:

1. Fit $l_1$:


    a. Split `edx` set into train and test sets randomly.
    b. Calculate $\mu_t$, $b_i$, and $b_u$ using the train set and different $l_1$s.
    c. Add $\mu_t$, $b_i$, and $b_u$ into the test set and calculate the RMSE.
    d. Repeat 4 more times and calculate average RMSEs for all $l_1$s and pick the $l_1$ with the lowest average RMSE.
$$ b_i = \left(\frac{\sum rating-\mu_t}{n_i+l_1}\right)$$
$rating$ stands for the label after stratification by movieId, $\mu_t$ is the overall rating average across all users and movies for each training set, $n_i$ is the rating count for movieId i and $l_1$ is the regularization term.
$$ b_u = \left(\frac{\sum rating-\mu_t-b_i}{n_u+l_1}\right)$$
$rating$ stands for the label after stratification by userId, $\mu_t$ is the overall rating average across all users and movies for each training set, $n_u$ is the rating count for userId u and $l_1$ is the regularization term.
```{r fit_L1,eval=FALSE,echo=FALSE}
# Find best l1 for b_u and b_i using helper function fit_bu_bi() and helper function partitionf().
# see apendix for details 
L1 <- seq(4,6,0.25)  # L1 values are carefully selected after some trial and error.
fit1<- fit_bu_bi(L1 = L1, tab = train)  # fit1 returns a vector of average RMSEs for each l1 value. 
l1 <- L1[which.min(fit1)]  # pick the l1 with the lowest average RMSE.
```
2. Fit $l_2$:

    a. Split `edx` set into train and test sets randomly.
    b. Using the train set, calculate $\mu_t$, $b_i$, and $b_u$ with the best fit $l_1$ obtained in step 1.
    c. Calculate $b_i(t)$ using the train set, $\mu_t$, $b_i$, $b_u$ values, and different $l_2$s.
    d. Add $\mu_t$, $b_i$, $b_u$, and $b_i(t)$ into the test set and calculate the RMSE.
    e. Repeat 4 more times and calculate average RMSEs for all $l_2$s and pick the $l_2$ with the lowest average RMSE.
 **It is important to note that in some cases the train sets may not have all of the movie and date combinations in the corresponding test sets. $b_i(t)$ will be set to zero in these situations.**
$$ b_i(t)= \left(\frac{\sum rating-\mu_t-b_i-b_u}{n_i(t)+l_2}\right)$$
$rating$ stands for the label after stratification by movieId and $date_q$, $\mu_t$ is the overall rating average across all users and movies for each training set, $n_i(t)$ is the rating count for movieId i during the $date_q$,t and $l_2$ is the regularization term.
```{r fit_L2,eval=FALSE,echo=FALSE}
# Calculate b_i and b_u using l1 and find best l2 for b_it using function fit_it() and helper function partitionf().
# see apendix for details 
L2 <- seq(48,55,0.5)  # L2 values are carefully selected after some trial and error.
fit2 <- fit_it(L2 = L2, tab = train,l1 = l1)  # fit2 returns a vector of average RMSEs for each l2 value. 
l2 <- L2[which.min(fit2)]  # pick the l2 with the lowest average RMSE.
```
3. Fit $l_3$:

    a. Split `edx` set into train and test set randomly.
    b. Using the train set, calculate $\mu_t$, $b_i$, $b_u$, and $b_i(t)$ with the best fit $l_1$ and $l_2$ from the previous steps.
    c. Calculate $b_u(t)$ using the train set, $\mu_t$, $b_i$, $b_u$, $b_i(t)$ values, and different $l_3$s.
    d. Add $\mu_t$, $b_i$, $b_u$, $b_i(t)$ and $b_u(t)$ into the test set and calculate the RMSE.
    e. Repeat 4 more times and calculate average RMSEs for all $l_3$s and pick the $l_3$ with the lowest average RMSE.
 **Similar to when calculating $b_i(t)$, the train sets may not have all of the user and date combinations in the corresponding test sets. $b_u(t)$ will be set to zero in those situations as well.**
$$ b_u(t)= \left(\frac{\sum rating-\mu_t-b_i-b_u-b_i(t)}{n_u(t)+l_3}\right)$$
$rating$ stands for the label after stratification by userId and $date_m$, $\mu_t$ is the overall rating average across all users and movies for each training set, $n_u(t)$ is the rating count for userId u during the $date_m$,t and $l_3$ is the regularization term.
```{r fit_L3,eval=FALSE,echo=FALSE}
# Calculate b_i and b_u using l1.
# Calculate b_it using l2.
# Find best l3 for b_ut using function fit_ut() and helper function partitionf().
# see apendix for details 
L3 <- seq(11,14,0.25)  # L3 values are carefully selected after some trial and error.
fit3 <- fit_ut(L3 = L3, tab = train,l1 = l1,l2 = l2)  # fit3 returns a vector of average RMSEs for each l3 value.
l3 <- L3[which.min(fit3)]  # pick the l3 with the lowest average RMSE.
```
4. Calculate $\mu$, $b_i$, $b_u$, $b_i(t)$ and $b_u(t)$ using the optimized $l_1$, $l_2$, $l_3$ for the `edx` set, and add the biases to the `validation` set. Zeros will be given to $b_i(t)$ and $b_u(t)$ if `validation` set doesn’t have the movie date or user date combinations.  
5. Calculate $\hat{B(u,i)}$ for the `validation` set using the baseline biases from step 4.
$$\hat{B(u,i)}= \mu+b_i+b_u+b_i(t)+b_u(t)$$
$\hat{B(u,i)}$ stands for the predicted baseline predictor for user u and movie i in the `validation` set.
```{r baseline_predictor_validation,eval=FALSE,echo=FALSE}
# Use helper function postprocess1() to calculate baseline biases using edx (train) and predict for validation (test).
# see apendix for details 
preSVD <- postprocess1(tab1 = train,tab2 = test,l1 = l1,l2 = l2,l3 = l3)  # contains modified test,train data frames and RMSE as a list
# remove test, and train set to save space.
rm(test,train) 
gc()
```
## Matrix Factorization:

We will use *funkSVD* from the *recommenderlab package* to calculate factor vectors for each user and movie. The *recommenderlab package* also contains convenient ways to convert data frames into realRatingMatrix. The detailed steps are below:

  1. Calculate the residuals for the `edx` set as shown below, then select the first column as userId, the second column as movieId and the third column as residuals.
$$R(u,i)=Rating(u,i)-B(u,i)$$
  2. Convert the 3 column data frame into a realRatingMatrix with rows as users, columns as movies, and residuals as “ratings”.
  3. The realRatingMatrix is then converted to a matrix format because *funkSVD* can only be performed on matrices.
  4. Run *funkSVD* on the matrix produced in step 3. The *funkSVD* function returns two matrices: `matrix U`, representing the user factor matrix; and `matrix V`, representing the movie factor matrix.
  5. Predict the $\hat{R(u,i)}$ for the `validation` set by taking the dot product of the factor vector for user u from `matrix U` with the movie i factor vector from `matrix V`.
$$ \hat{R(u,i)} = U_u \bullet V_i $$
$\hat{R(u,i)}$ stands for the predicted residual for user u and movie i in the `validation` set.
```{r SVD,eval=FALSE,echo=FALSE}
# use helper function SVDf() to get movie and user factor matrices. 
# recommenderlab is loaded inside helper function SVDf()
# see appendix for details
movie_user_feature<- SVDf(tab = preSVD$train)  # contains movie factor matrix and user factor matrix as a list
```
## Final Prediction:

The predicted rating for the `validation` set will be the sum of $\hat{B(u,i)}$ and $\hat{R(u,i)}$:
$$\hat{Rating(u,i)}=\hat{B(u,i)} + \hat{R(u,i)}$$
$\hat{Rating(u,i)}$ stands for the predicted rating for user u and movie i in the `validation` set.
```{r final_Prediction,eval=FALSE,echo=FALSE}
# Use helper function finalprocess() to calculate predicted residual and rating for the validation set and the final RMSE. 
# see appendix for details
finalresult <- finalprocess(tab = preSVD$test, movief = movie_user_feature$movief,userf = movie_user_feature$userf)  # a list containing predicted rating and final RMSE
# Remove movie_user_feature and preSVD to save space
rm(preSVD,movie_user_feature)
gc()
# Final predicted rating for validation set
pred <- finalresult$pred
# Final rmse for validation set
RMSE <- finalresult$rmse
# remove finalresult to save space
rm(finalresult)
gc()
```
# Results:
```{r enter_results,echo=FALSE}
# to speedup report process, manually entered the information 
L1 = seq(4,6,0.25)
fit1 = c(0.8649230,0.8649166,0.8649124,0.8649103,0.8649101,0.8649117,0.8649152,0.8649204,0.8649272)
L2 = seq(48,55,0.5)
fit2 = c(0.8592583,0.8592574, 0.8592567, 0.8592561, 0.8592556, 0.8592552, 0.8592548,
         0.8592546, 0.8592545, 0.8592545, 0.8592545, 0.8592546, 0.8592549, 0.8592552, 0.8592555)
L3 = seq(11,14,0.25)
fit3 = c(0.8545379, 0.8545356, 0.8545336, 0.8545320, 0.8545309, 0.8545301, 0.8545296,
         0.8545294, 0.8545296, 0.8545300, 0.8545307, 0.8545317, 0.8545329)
preSVD=0.8541734
finalRMSE = 0.7943898
```
## Baseline predictors:  
Below are results of baseline predictor model fittings for $b_i$, $b_u$, $b_i(t)$ and $b_u(t)$. The x axes are $L_1$, $L_2$ and $L_3$ values; and the y axes are the corresponding average RMSEs: 
```{r l1_l2_l3_fit_plot,fig.height =5,echo=FALSE}
p11<- qplot(L1,fit1)+geom_point(col='red',size=2)+ylab('average RMSEs')
p12<- qplot(L2,fit2)+geom_point(col='darkblue',size=2)+ylab('average RMSEs')
p13<- qplot(L3,fit3)+geom_point(col='darkgreen',size=2)+ylab('average RMSEs')
grid.arrange(p11,p12,p13,ncol=2,nrow=2)
```  
## RMSE Comparisons:
Here is a table comparing diffrent RMSEs after each sub model fitting. **Note that the first 3 RMSEs are the lowest average RMSEs after fitting $l_1$, $l_2$ and $l_3$ using only the `edx` random test data sets.** Adding **temporal effects** reduces the average RMSE for the `edx` random test data sets by over 0.01 and **Matrix Factorization** reduces the `validation` RMSE by over 0.06.
```{r compare_rmses,echo=FALSE}
data.frame(models=c('constant movie user biases','movie temporal bias added','movie user temporal biases added',
                    'validation baseline predictor','validation residual added'),
           RMSE=c(min(fit1),min(fit2),min(fit3),preSVD,finalRMSE))%>%kable()%>%
  kable_styling(position = 'center')%>%
  column_spec(1,border_left = T)%>%column_spec(2,border_right = T)%>%
  row_spec(0,bold=T)
```

# Conclusions:

After using both **baseline predictors** and **matrix factorization**, we are able to achieve an RMSE of `r round(finalRMSE,3)`, well below the requirement for this course project. That being said, the prediction is off by almost one star. More improvements can be made by including *frequency* as part of the **baseline predictors** for user and movie biases, performing **matrix factorization** with **temporal dynamics**, and using **Restricted Boltzmann Machines** for inactive users and unpopular movies. **Gradient Boosted Decision Trees** could also provide improvement by acting as a *blending scheme* with extra features such as *number of ratings for each movie*, *number of movies rated by each user*, *days since the movies first rated* and *days since the user first rated a movie*. Recommendation systems are a very complex machine learning problem with many unpredictable user and movie interactions. That being said, with an RMSE of `r round(finalRMSE,3)` on ratings, Netflix should be able to predict if a specific user would love or hate a specific movie.

# Appendix:

**Helper Functions**:
To save memory and to speed up computation, 9 different helper functions are created for this project along with removing unnecessary variables followed by garbage collections:

* Helper function 1: performs the data preprocess described in the **Data Preprocessing** section.
```{r helperfunction_1,eval=FALSE,echo=FALSE}
preprocess <- function(tab){
  # helper function 1, remove genres, titles columns, and convert timestamp column into date.
  # round the date into monthly for b_ut(date_m) and quarterly for b_it (date_q).
  # remove timestamp and date column. 
  #
  # Args:
  #   tab: either edx data frame or validation data frame.
  #
  # Returns:
  #   Transformed edx data frame or validation data frame. 
  tab <- tab%>%select(-c('title','genres'))%>%
    mutate(date=as_datetime(timestamp))%>%
    mutate(date_q=round_date(date, unit='quarter'), date_m=round_date(date,unit='month'))%>%
    select(-c('date','timestamp'))
  return (tab)
}
}
```
* Helper function 2: `edx` data partition function to allow cross validation for all three regularization terms.
```{r helperfunction_2,eval=FALSE,echo=FALSE}
partitionf <- function(tab){
  # helper function 2, randomly divide edx into train (~ 90% edx) and test (~ 10% edx).
  # make sure train contains all userId and movieId in test set.
  #
  # Args:
  #   tab: transformed edx data frame from function preprocess() with 5 columns(userId, movieId, rating, date_q, date_m).
  #
  # Returns:
  #   a list with 2 data frames, test data frame and train data frame.
  test_index <- createDataPartition(y = tab$rating, times = 1, p = 0.1, list = FALSE)
  train <- tab[-test_index,]
  temp <- tab[test_index,]
  # Make sure userId and movieId in test set are also in train set
  test <- temp %>% 
    semi_join(train, by = "movieId") %>%
    semi_join(train, by = "userId")
  # Add rows removed from test set back into train set
  removed <- anti_join(temp, test)
  train <- rbind(train, removed)
  return (list(train=train,test=test))
}
```
* Helper function 3 to 5: optimization functions for the three regularization terms. Each function returns a numeric vector of average RMSEs.
```{r helperfunction_3_to_5,eval=FALSE,echo=FALSE}
fit_bu_bi<- function(L1, tab){
  # helper function 3, function to help find best l1 for b_u and b_i.
  # cross validates 5 times (sampling using helper function 2 ,partitionf() for 5 times).
  # average RMSEs across 5 randomly generated test sets for each l1 value will be calculated. 
  #
  # Args:
  #   L1: a numeric vector of regularization terms for b_i and b_u.
  #   tab: transformed edx data frame from function preprocess() with 5 columns(userId, movieId, rating, date_q, date_m).
  #
  # Returns:
  #   a numeric vector of average RMSEs for each l1 value.
  rmses<- replicate(5,{
    t <- partitionf(tab = tab)  # generate a random test and train set 
    mu <- mean(t$train$rating)  # average rating for each new train set from partitionf()  
    sapply(L1,function(l1){
      # calculate b_i and b_u on the train set 
      b_i <- t$train%>%
        group_by(movieId)%>%
        summarize(b_i=sum(rating-mu)/(n()+l1))
      b_u <- t$train%>%
        left_join(b_i, by='movieId')%>%
        group_by(userId)%>%
        summarize(b_u=sum(rating-mu-b_i)/(n()+l1))
      # add b_i and b_u to the test set to calculate predicted rating 
      p <- t$test%>%
        left_join(b_i,by='movieId')%>%
        left_join(b_u,by='userId')%>%
        mutate(p = mu+b_i+b_u)%>%.$p
      # caret package already has an RMSE function ( sqrt(mean(truevalue-predvalue)^2)))
      return(RMSE(t$test$rating,p)) 
    }
    )
  })
  # RMSEs are returned as a matrix (each row is for l1 value and each column is for each test sample set)
  # average RMSEs will be the average row value across all test samples 
  avg_rmses <- rowMeans(rmses) 
  return(avg_rmses)
}

fit_it <- function(L2, tab, l1){
  # helper function 4, function to help find best l2 for b_it.
  # cross validates 5 times (sampling using helper function 2 ,partitionf() for 5 times).
  # average RMSEs across 5 randomly generated test sets for each l2 value will be calculated. 
  #
  # Args:
  #   L2: a numeric vector of regularization terms for b_it.
  #   tab: transformed edx data frame from function preprocess() with 5 columns(userId, movieId, rating, date_q, date_m).
  #   l1: a numeric value of regularization term for b_i and b_u.
  #
  # Returns:
  #   a numeric vector of average RMSEs for each l2 value.
  rmses <- replicate(5,{
    t <- partitionf(tab = tab)  #generate a random test and train set 
    mu <- mean(t$train$rating)  #average for each new train set using partitionf()  
    # use l1 to fit b_i and b_u on each new train set 
    b_i <- t$train%>%
      group_by(movieId)%>%
      summarize(b_i=sum(rating-mu)/(n()+l1)) 
    b_u <- t$train%>%left_join(b_i, by='movieId')%>%
      group_by(userId)%>%summarize(b_u=sum(rating-mu-b_i)/(n()+l1)) 
    # add b_i and b_u to each train set before sapply to speed up the process
    train<- t$train%>%
      left_join(b_i, by='movieId')%>%left_join(b_u, by='userId')
    # add b_i and b_u to each test before sapply to speed up the process
    test <- t$test%>%
      left_join(b_i,by='movieId')%>%left_join(b_u,by='userId')
    sapply(L2,function(l2){
      # calculate b_it for the train set 
      b_it <- train%>%
        group_by(movieId, date_q)%>%
        summarize(b_it=sum(rating-mu-b_i-b_u)/(n()+l2))
      # add b_it to test set to calculate predicted rating 
      # for some movies that didn't get rated during that quarter will get a zero as no information during that period 
      p <- test%>%left_join(b_it,by=c('movieId','date_q'))%>%
        mutate(b_it=ifelse(is.na(b_it),0,b_it))%>%
        mutate(p = mu+b_i+b_u+b_it)%>%.$p
      # using RMSE in caret package 
      return(RMSE(test$rating,p))
    }
    )
  }
  )
  # RMSEs are returned as a matrix (each row is for l2 value and each column is for each test sample set)
  # average RMSEs will be the average row value across all test samples 
  avg_rmses <- rowMeans(rmses) 
  return(avg_rmses)
}

fit_ut <- function(L3, tab, l1, l2){
  # helper function 5, function to help find best l3 for b_ut.
  # cross validates 5 times (sampling using helper function 2 ,partitionf() for 5 times).
  # average RMSEs across 5 randomly generated test sets for each l3 value will be calculated. 
  #
  # Args:
  #   L3: a numeric vector of regularization terms for b_ut.
  #   tab: transformed edx data frame from function preprocess() with 5 columns(userId, movieId, rating, date_q, date_m).
  #   l1: a numeric value of regularization term for b_i and b_u.
  #   l2: a numeric value of regularization term for b_it.
  #
  # Returns:
  #   a numeric vector of average RMSEs for each l3 value.
  rmses <- replicate(5,{ 
    t <- partitionf(tab = tab)  #generate a random test and train set 
    mu <- mean(t$train$rating)  #average for each new train set using partitionf()  
    # use l1 to fit b_i and b_u on each new train set 
    b_i <- t$train%>%
      group_by(movieId)%>%summarize(b_i=sum(rating-mu)/(n()+l1))
    b_u <- t$train%>%left_join(b_i, by='movieId')%>%
      group_by(userId)%>%summarize(b_u=sum(rating-mu-b_i)/(n()+l1))
    # add b_i and b_u to each train set before sapply to speed up the proces
    train <- t$train%>%left_join(b_i, by='movieId')%>%left_join(b_u, by='userId')
    # use l2 to calculate b_it for each new train set
    b_it <- train%>%group_by(movieId,date_q)%>%
      summarize(b_it=sum(rating-mu-b_i-b_u)/(n()+l2))
    # add b_it to each train set before sapply to speed up the process 
    train<- train%>%
      left_join(b_it, by=c('movieId','date_q'))
    # add b_i, b_u, and b_it to each test set before sapply to speed up the process
    # for some movies that didn't get rated during that quarter will get a zero as no information during that period 
    test <- t$test%>%
      left_join(b_i,by='movieId')%>%left_join(b_u,by='userId')%>%
      left_join(b_it, by=c('movieId','date_q'))%>%
      mutate(b_it=ifelse(is.na(b_it),0,b_it))
    sapply(L3,function(l3){
      # calculate b_ut on the train set
      b_ut <- train%>%
        group_by(userId, date_m)%>%
        summarize(b_ut=sum(rating-mu-b_i-b_u-b_it)/(n()+l3))
      # add b_ut to the test set to calculate predicted rating 
      # for some users that didn't rate during that month will get a zero as no information during that period 
      p <- test%>%
        left_join(b_ut,by=c('userId','date_m'))%>%
        mutate(b_ut=ifelse(is.na(b_ut),0,b_ut))%>%
        mutate(p = mu+b_i+b_u+b_it+b_ut)%>%.$p
      # using RMSE in caret package 
      return(RMSE(test$rating,p)) 
    }
    )
  }
  )
  # RMSEs are returned as a matrix (each row is for l3 value and each column is for each test sample set)
  # average RMSEs will be the average row value across all test samples 
  avg_rmses <- rowMeans(rmses) 
  return(avg_rmses) 
}
```
* Helper function 6: calculates the predicted baseline biases for user u and movie i in the `validation` set, and returns `edx` as a 3 column data frame consisting of userId, movieId and residuals for **matrix factorization**, `validation` data frame with only userId, movieId and predicted baseline biases for final rating prediction and the RMSE value without **matrix factorization**.
```{r helperfunction_6,eval=FALSE,echo=FALSE}
postprocess1<-function(tab1,tab2,l1,l2,l3){
  # helper function 6, use the best overall l1,l2,and l3 to calculate b_i, b_u, b_it, and b_ut. 
  # calculates preSVD predicted rating and RMSE for validation set. 
  #
  # Args:
  #   tab1: transformed edx data frame from function preprocess() with 5 columns(userId, movieId, rating, date_q, date_m).
  #   tab2: transformed validation data frame from function preprocess() with 5 columns(userId, movieId, rating, date_q, date_m).
  #   l1: a numeric value of regularization term for b_i and b_u.
  #   l2: a numeric value of regularization term for b_it.
  #   l3: a numeric value of regularization term for b_ut.
  #
  # Returns:
  #   a list of 3 items, train data frame, test data frame, and rmse
  #   train, transformed tab1 with 3 columns(userId, movieId, and calculated residual (true rating - predicted rating)).
  #   test, transformed tab2 with 4 columns(userId, movieId, actual rating, and preSVD predicted rating).
  #   rmse, a numeric value of RMSE for the validation set without SVD
  mu <- mean(tab1$rating)  # calculate the mean using tab1 
  # calculate b_i and b_u using tab1 and l1
  b_i <- tab1%>%
    group_by(movieId)%>%summarize(b_i=sum(rating-mu)/(n()+l1))
  b_u <- tab1%>%
    left_join(b_i, by='movieId')%>%
    group_by(userId)%>%summarize(b_u=sum(rating-mu-b_i)/(n()+l1))
  # add b_i and b_u to tab1 and save as train
  train <- tab1%>%
    left_join(b_i, by='movieId')%>%
    left_join(b_u, by='userId')
  # calculate b_it using train(modified tab1) and l2
  b_it <- train%>%
    group_by(movieId,date_q)%>%
    summarize(b_it=sum(rating-mu-b_i-b_u)/(n()+l2))
  # add b_it to train
  train <- train%>%
    left_join(b_it, by=c('movieId','date_q'))
  # calculate b_ut using train and l3
  b_ut <- train%>%
    group_by(userId, date_m)%>%
    summarize(b_ut=sum(rating-mu-b_i-b_u-b_it)/(n()+l3))
  # add b_ut to train 
  # calculate residual (true rating - predicted rating)
  # select userId, movieId, and residual to make matrix later for SVD using recommenderlab 
  train <- train%>%
    left_join(b_ut, by=c('userId','date_m'))%>%
    mutate(p = mu+b_i+b_u+b_it+b_ut)%>%
    mutate(resid=rating-p)%>%
    select(userId,movieId,resid)
  # add b_i,b_u,b_it,b_ut to tab2 and save as test 
  # for b_it and b_ut, 0 is given when no information during that period 
  # calulate predicted rating using just base predictor biases
  # select userId, movieId, predicted rating and actual rating for final prediction
  test<- tab2%>%
    left_join(b_i, by='movieId')%>%
    left_join(b_u, by='userId')%>%
    left_join(b_it, by=c('movieId','date_q'))%>%
    mutate(b_it=ifelse(is.na(b_it),0,b_it))%>%
    left_join(b_ut,by=c('userId','date_m'))%>%
    mutate(b_ut=ifelse(is.na(b_ut),0,b_ut))%>%
    mutate(p = mu+b_i+b_u+b_it+b_ut)%>%select(userId,movieId,p,rating) 
  # calculate preSVD RMSE for validation set using caret package 
  return (list(train=train,test=test,rmse=RMSE(test$rating,test$p))) 
}
```
* Helper function 7: converts `edx` data frame from `helper function 6` into matrix, performs *funkSVD*, and returns movie and user factor matrices.
```{r helperfunction_7,eval=FALSE,echo=FALSE}
SVDf <- function(tab){
  # helper function 7, perform funkSVD to calculate U and V feature factors.
  # calculates 10 features for each user and each movie.
  #
  # Args:
  #   tab: transformed edx data frame, train, using function postprocess1() with 3 columns (userId, movieId, and calculated residual).
  #
  # Returns:
  #   a list of 2 items, movie feature matrix, movief, and user feature matrix, userf.
  library(recommenderlab)  #load the recommenderlab package
  # convert tab into a matrix (row as userId, column as movieId, each matrix value as residual)
  resid_matrix <- tab%>%as('realRatingMatrix')%>%as('matrix')
  # set max iteration to 300 and set verbose to true to see real time progress 
  # caution: don't run this, it will take one day and it may crash your computer! 
  fsvd <- funkSVD(resid_matrix,max_epochs = 300,verbose = TRUE)
  # get movie matrix from fsvd (each row is for movieId and 10 columns for 10 feature factors)
  movief <- fsvd$V
  # set the correct moiveId for movie feature matrix
  rownames(movief)<-colnames(resid_matrix)
  # get user matrix from fsvd (each row is for userId and 10 columns for 10 feature factors)
  userf <- fsvd$U
  # set the correct userId for user feature matrix 
  rownames(userf)<-rownames(resid_matrix) 
  return(list(movief=movief,userf=userf))
}
```
* Helper function 8: find correct user factor vector and correct movie factor vector by matching userId and movieId and perform vector dot product to calculate predicted residuals for user u and movie i. **`Helper function 8` is inside the `helper function 9` in the `R script`.**
* Helper function 9: uses `helper function 8` to calculate predicted residuals for user u and movie i in the `validation` set, calculates the final predicted ratings, and returns the predicted ratings along with the final RMSE.
```{r helperfunction_8_&_9,eval=FALSE,echo=FALSE}
finalprocess <- function(tab, movief, userf){
  # helper function 9, calculates the final predicted rating and final RMSE for the validation set
  #
  # Args:
  #   tab: transformed validation data frame, test, using function postprocess1() with 4 columns(userId, movieId, actual rating, and preSVD predicted rating).
  #   movief: movie feature matrix using function SVDf().
  #   userf: user feature matrix using function SVDf().
  # Returns:
  #   a list of 2 items, predicted rating, pred, and final RMSE, rmse.
  #   pred is a large numeric vector of final predicted rating for the validation set.
  #   rmse is the final RMSE for the validation set.
  # Create helperfunction 8 to calculate specific user and movie residual
  resid_f<- function(u,i){
    # helperfunction 8, calculates predicted residual for user u and movie i using vector dot product.
    #
    # Args:
    #   u: a numeric value of userId.
    #   i: a numeric value of movieId.
    #
    # returns:
    #   a numeric value of residual (user factor vector dot product movie factor vector). 
    u <-userf[toString(u),]  # create user factor vector by matching userId with correct rowname
    i <- movief[toString(i),]  # create movie factor vector by matching movieId with correct rowname
    # calculates residual and returns the numeric value
    return(sum(u*i))
  }
  # Use mapply to use function resid_f() to calculate predicted residual and final predicted rating.
  test <-tab%>%
    mutate(resid = mapply(resid_f,userId,movieId))%>%
    mutate(p=p+resid)
  # calculates final RMSE using caret package
  return(list(pred=test$p,rmse=RMSE(test$rating,test$p)))
}
```
